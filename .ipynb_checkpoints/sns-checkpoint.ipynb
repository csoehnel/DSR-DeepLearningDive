{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an image classifier able to find sneakers in instagram posts\n",
    "\n",
    "The data comprises of few thousand images of sneakers collected using google images and instagram\n",
    "and few thousand images of sneakers.    \n",
    "Your goal is to use what you learned from previous examples and create a sneaker-not-sneaker binary classifier.\n",
    "\n",
    "The task comprises of multiple sub-tasks that you need to do to build the classifier.\n",
    "\n",
    "1. Create a dataset able to load data from new_meta_sneakers.csv\n",
    "2. Create a fine tune binary classification architecture.\n",
    "3. Create a training loop and train your model.\n",
    "\n",
    "![title](sneakers.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the bottom of the following cell you see the data you will work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torch import nn\n",
    "import easyimages\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pretrainedmodels.models import resnet50\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET_USAGE_PERCENTAGE = 0.1\n",
    "\n",
    "base_path ='/media/i008/ssd500/fashion_classify_data/'\n",
    "df = pd.read_csv(os.path.join(base_path,'new_meta_sneakers.csv')).sample(frac=1)\n",
    "df.image_path = base_path +df.image_path\n",
    "df.tags = df.tags.map({'sneakers': 1, 'negatives': 0})\n",
    "df = df.sample(frac=DATASET_USAGE_PERCENTAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the torch Dataset.\n",
    "\n",
    "First thing we need to do is create a dataset able to load our data. Since our metadata is stored in a csv file, our \n",
    "dataset should accept this file as a base source of what needs to be loaded.\n",
    "\n",
    "Our dataset should also support augumentations and a \"inference\" mode wich disables them for predicting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "class OneClassImageClassificationDataset(Dataset):\n",
    "    def __init__(self, annotations, image_transform):\n",
    "        \"\"\"\n",
    "        annotations is a pandas dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.annotations = annotations\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the annotations dataframe\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Using methods you wrote:\n",
    "        1 - load image from disk for given index  (self.load_from_disk)\n",
    "        2 - transform image (self.image_transform)\n",
    "        3 - Load target (self.load_target)\n",
    "        return Xi, yi\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        Xi = self.load_from_disk(index)\n",
    "        Xi = self.image_transform(Xi)\n",
    "        yi = self.load_target(index)\n",
    "        return Xi, yi\n",
    "\n",
    "    def load_to_pil(self, uri):\n",
    "        \"\"\"\n",
    "        Write a helper function that uses PIL.Image to load a file and returns it\n",
    "        \"\"\"\n",
    "\n",
    "        image_pil = Image.open(uri)\n",
    "        image_pil = image_pil.convert(\"RGB\")\n",
    "        # image_pil = YOUR CODE HERE\n",
    "        return image_pil\n",
    "\n",
    "\n",
    "    def load_from_disk(self, index):\n",
    "        \"\"\"\n",
    "        Loads an image from disk given a index.\n",
    "        It gets the path of an image with the corresponding index from the metadata \n",
    "        It passes the URI to the self.load_to_pil and returns a PIL.Image\n",
    "        \"\"\"\n",
    "        image_path = self.annotations.iloc[index]['image_path']\n",
    "        #image_path = # YOUR CODE HERE\n",
    "        return self.load_to_pil(image_path)\n",
    "\n",
    "    def load_target(self, index):\n",
    "        \"\"\"\n",
    "        This function should get the tag for a given index from the annotations dataframe\n",
    "        You .iloc can become useful.    \n",
    "        This methods should return, either a 0 or a 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        #label = # YOUR CODE HERE\n",
    "        label = self.annotations.iloc[index]['tags']\n",
    "\n",
    "        return label\n",
    "    \n",
    "    \n",
    "class BaseSampler(Sampler):\n",
    "    def __init__(self, df, n_samples):\n",
    "        self.df = df\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(self._get_sample())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def _get_sample(self):\n",
    "        return np.random.choice(len(self.df), self.n_samples, replace=False)\n",
    "        \n",
    "\n",
    "def binary_classification_model():\n",
    "    \"\"\"\n",
    "    Write a function that loads a resnet50 model from pretrainedmodels, freezes its layers\n",
    "    replaces the last_linear with the proper output number. As we did in previous example.\n",
    "    replace avgpool with adaptiv pooling.\n",
    "    \"\"\"\n",
    "    model = resnet50()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    inft = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_features=inft, out_features=2)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    \n",
    "    # model = YOUR CODE HERE\n",
    "    return model\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "aug_seq = iaa.Sequential([\n",
    "    iaa.Fliplr(p=0.5),\n",
    "    iaa.Sometimes(\n",
    "        0.3,\n",
    "        iaa.Multiply((0.9, 1.2))\n",
    "    ),\n",
    "    iaa.Sometimes(\n",
    "        0.3,\n",
    "        iaa.AdditiveGaussianNoise()\n",
    "    ),\n",
    "    iaa.Affine(\n",
    "        scale=(0.5, 2),\n",
    "        translate_percent=(-0.2, 0.2)\n",
    "    )\n",
    "])\n",
    "def augment(self, augmenter, image):\n",
    "    augmenter = augmenter.to_deterministic()\n",
    "    img_aug = augmenter.augment_image(np.array(image))\n",
    "    img_aug = Image.fromarray(img_aug)\n",
    "    return img_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# YOUR CODE HERE:\n",
    "# SPLIT the dataframe into df_train, df_test (thing about using sklearn.model_selection.train_test_split)\n",
    "df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "df_train = df_train.reset_index()\n",
    "df_test = df_test.reset_index()\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "image_transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "# YOUR CODE define image_transform_test\n",
    "image_transform_test = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "# YOUR CODE define the crieterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net = binary_classification_model()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# YOUR CODE\n",
    "# Instantiate the OneClassImageClassificationDatasets\n",
    "train_ds = OneClassImageClassificationDataset(df_train, image_transform=image_transform_train)\n",
    "test_ds = OneClassImageClassificationDataset(df_test, image_transform=image_transform_test)\n",
    "\n",
    "# initialize the BaseSampler\n",
    "bs = BaseSampler(train_ds, 1000)\n",
    "\n",
    "#YOUR CODE\n",
    "#Initialize your DataLoader (using datasets)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=bs)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def evaluate_model(model, loader, print_info=False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        collect_results = []\n",
    "        collect_target = []\n",
    "        for batch in loader:\n",
    "            X, y = batch\n",
    "            X = X.to(DEVICE)\n",
    "            y = y.to(DEVICE).detach().cpu().numpy()\n",
    "            pred = model(X)\n",
    "            collect_results.append(pred.sigmoid().detach().cpu().numpy())\n",
    "            collect_target.append(y) \n",
    "    \n",
    "        preds_proba = np.concatenate(collect_results)\n",
    "        preds = preds_proba.argmax(axis=1)\n",
    "        \n",
    "        targets = np.concatenate(collect_target)\n",
    "        \n",
    "        ll = log_loss(targets, preds_proba)\n",
    "        acc = accuracy_score(targets, preds)\n",
    "        if print_info:\n",
    "            print(\"test log-loss: {}\".format(ll))\n",
    "            print(\"overall accuracy:  {}\".format(acc))\n",
    "            #print(classification_report(targets, preds))\n",
    "        model.train()\n",
    "        \n",
    "        return ll, acc\n",
    "    \n",
    "metrics = []\n",
    "metrics_names = ['loss_train','loss_test','acc_train','acc_test']\n",
    "losses = []\n",
    "\n",
    "net.to(DEVICE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for X, y in train_dl:\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        ypred=net(X)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "    testll, testacc = evaluate_model(net, test_dl)\n",
    "    trainll, trainacc = evaluate_model(net, train_dl)\n",
    "    print(\"test: {} {}\".format(testll, testacc))\n",
    "    print(\"train: {} {}\".format(trainll, trainacc))\n",
    "    metrics.append([trainll, testll, trainacc, testacc])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
