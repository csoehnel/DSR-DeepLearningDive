{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from albumentations import (\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,\n",
    "    Compose,\n",
    "    RandomRotate90,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    RandomGamma\n",
    ")\n",
    "\n",
    "from metrics import dice\n",
    "from data import NeuronSegmDataset\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from encoders import ResNetEncoders\n",
    "from unet import UNet\n",
    "\n",
    "def tonp(t):\n",
    "    return t.detach().cpu().numpy()\n",
    "    \n",
    "def process_batch(b):\n",
    "    X, y  = b\n",
    "    X = X.to(DEVICE)\n",
    "    y = torch.tensor(y, dtype=torch.float).to(DEVICE)\n",
    "    return X, y\n",
    "\n",
    "def prediction_to_numpy(pred):\n",
    "    return pred.sigmoid().detach().cpu().numpy() \n",
    "\n",
    "def evaluate(model, loader, threshold):\n",
    "    model = model.eval()\n",
    "    for b in loader:\n",
    "        X, y = process_batch(b)\n",
    "        X = X.repeat(1, 3, 1, 1)\n",
    "        res = model(X)\n",
    "    val_loss = nn.BCEWithLogitsLoss()(res, y)\n",
    "    pred_Y, real_Y = prediction_to_numpy(res) > threshold, tonp(y)> threshold\n",
    "    dices = []\n",
    "    for ypred, yreal in zip(pred_Y, real_Y):\n",
    "        dices.append(dice(ypred, yreal))\n",
    "    return np.array(dices).mean(), val_loss.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_target = './train-labels.tif'\n",
    "neuron_train = './train-volume.tif'\n",
    "\n",
    "\n",
    "trans_train = transforms.Compose([transforms.Resize(192), \n",
    "                                  transforms.ToTensor()])\n",
    "trans_test = transforms.Compose([transforms.Resize(192), \n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "aug = Compose([RandomGamma(), \n",
    "               VerticalFlip(), \n",
    "               HorizontalFlip(),\n",
    "               RandomRotate90(), \n",
    "               GridDistortion(),\n",
    "               ElasticTransform(),\n",
    "               OpticalDistortion(distort_limit=1, shift_limit=1)])\n",
    "\n",
    "\n",
    "train_ds = NeuronSegmDataset(neuron_train, neuron_target, image_transform=trans_train, augmenter=aug)\n",
    "test_ds = NeuronSegmDataset(neuron_train, neuron_target, image_transform=trans_test)\n",
    "train_ds_val = NeuronSegmDataset(neuron_train, neuron_target, image_transform=trans_train)\n",
    "\n",
    "\n",
    "random_training_sampler = RandomSampler(train_ds)\n",
    "train_idx , test_idx = train_test_split(range(len(train_ds)), test_size=0.15)\n",
    "\n",
    "random_sampler_train = RandomSampler(train_idx)\n",
    "sampler_test = SequentialSampler(test_idx)\n",
    "\n",
    "training_data_loader = DataLoader(train_ds, batch_size=1, num_workers=1, sampler=random_training_sampler)\n",
    "test_data_loader = DataLoader(test_ds, batch_size=6, num_workers=1, sampler=sampler_test)\n",
    "train_ds_val_loader = DataLoader(train_ds_val, batch_size=6, num_workers=1, sampler=random_training_sampler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# unet = OriginalUnet(n_classes=1,depth=3, padding=True, up_mode='upsample').cuda()\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "resnet = ResNetEncoders(18, pretrained=True).to(DEVICE)\n",
    "unet = UNet(resnet, 1).to(DEVICE)\n",
    "\n",
    "optim = torch.optim.Adam(unet.parameters())\n",
    "epochs = 1000\n",
    "loss_bce = nn.BCEWithLogitsLoss()\n",
    "loss = 0\n",
    "THRESHOLD = 0.5\n",
    "history = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _ in enumerate(range(epochs)):\n",
    "    for batch in training_data_loader:\n",
    "        X, y = process_batch(batch)\n",
    "        X = X.repeat(1,3,1,1)\n",
    "        prediction = unet(X)  # [N, 2, H, W]\n",
    "        loss = loss_bce(prediction, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()   \n",
    "        \n",
    "    val_mean_dice, val_loss = evaluate(unet, test_data_loader, threshold=THRESHOLD)\n",
    "    train_mean_dice, train_loss = evaluate(unet, train_ds_val_loader, threshold=THRESHOLD)\n",
    "    \n",
    "    print(\"epoch\", i)\n",
    "    print('dice', val_mean_dice)\n",
    "    print('loss', val_loss)\n",
    "    print('train_dice', train_mean_dice)\n",
    "    print('train_loss', train_loss)\n",
    "    unet.train()\n",
    "    \n",
    "    history['val_dice'].append(val_mean_dice)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_dice'].append(train_mean_dice)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(history)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('train/val dice')\n",
    "plt.plot(df.val_dice)\n",
    "plt.plot(df.train_dice)\n",
    "plt.legend(['val_dice', 'train_dice'])\n",
    "plt.figure()\n",
    "plt.title('val/train loss')\n",
    "plt.plot(df.val_loss)\n",
    "plt.plot(df.train_loss)\n",
    "plt.legend(['val_loss', 'train_loss'])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(prediction[0][0].detach().sigmoid().cpu().numpy(), cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(y.detach()[0][0].cpu().numpy(), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load .alaunet.py\n",
    "\n",
    "class AlaUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlaUnet, self).__init__()\n",
    "\n",
    "        \n",
    "        # Create 4 conv layers with 16, 32, 64, 16 inputs\n",
    "        # Create a Linear layer that will map this into a 192x192 vector\n",
    "        # Assume input shape to be (1, 1, 192, 192)\n",
    "        \n",
    "        \n",
    "        self.layer0 = nn.Sequential(nn.Conv2d(1, 4, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.layer1_up_stream = nn.Sequential(nn.Conv2d(48, 16, kernel_size=1), nn.ReLU())  \n",
    "        self.layer2_up_stream = nn.Sequential(nn.Conv2d(24, 20, kernel_size=1), nn.ReLU())\n",
    "        self.layer3_up_stream = nn.Sequential(nn.Conv2d(24, 16, kernel_size=1), nn.ReLU())\n",
    "        self.layer4_up_stream = nn.Sequential(nn.Conv2d(17, 1, kernel_size=1), nn.ReLU())\n",
    "\n",
    "        \n",
    "        self.linear = nn.Linear(16 * 12 * 12, 36864)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        down0 = self.layer0(x)\n",
    "        down1 = self.layer1(down0)\n",
    "        down2 = self.layer2(down1)\n",
    "        down3 = self.layer3(down2)\n",
    "        down4 = self.layer4(down3)\n",
    "        \n",
    "        up1 = self.upsample(down4)\n",
    "        \n",
    "        first_up = torch.cat([up1, down3], dim=1)\n",
    "        up2 = self.upsample(self.layer1_up_stream(first_up))\n",
    "        \n",
    "        second_up = torch.cat([up2, down2], dim=1)\n",
    "        up3 = self.upsample(self.layer2_up_stream(second_up))\n",
    "        \n",
    "        third_up = torch.cat([up3, down1],dim=1)\n",
    "        up4 = self.upsample(self.layer3_up_stream(third_up))\n",
    "        \n",
    "        \n",
    "        x = self.layer4_up_stream(torch.cat([x, up4], dim=1))\n",
    "        \n",
    "        \n",
    "        return x\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load .ssn.py\n",
    "\n",
    "class SSN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SSN, self).__init__()\n",
    "\n",
    "        \n",
    "        # Create 4 conv layers with 16, 32, 64, 16 inputs\n",
    "        # Create a Linear layer that will map this into a 192x192 vector\n",
    "        # Assume input shape to be (1, 1, 192, 192)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(1 * 12 * 12, 36864)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        after_conv =self.layer4(self.layer3(self.layer2(self.layer1(x))))\n",
    "        flatten = after_conv.view(after_conv.shape[0], -1)\n",
    "\n",
    "        lin = self.linear(flatten)\n",
    "        \n",
    "    \n",
    "        return lin.view(x.shape[0],1,192,192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
