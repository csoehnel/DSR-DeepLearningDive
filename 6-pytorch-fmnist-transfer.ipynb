{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.6/site-packages/easyimages/utils.py:7: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 499, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 810, in start\n",
      "    self._run_callback(callback)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 592, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 963, in <lambda>\n",
      "    self.future, lambda f: self.run())\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 879, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 346, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 230, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 259, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 230, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 513, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 230, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-22a77da2cdb3>\", line 7, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/home/i008/anaconda3/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  mpl.use('agg')\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms \n",
    "import pandas as pd\n",
    "import PIL\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from easyimages import EasyImageList\n",
    "from torch import nn\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from pretrainedmodels.models import resnet18, resnet50\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For details related to dataloading check the previous notebook (pytorch-fmnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    " 0: 'T-shirt/top',\n",
    " 1: 'Trouser',\n",
    " 2: 'Pullover',\n",
    " 3: 'Dress',\n",
    " 4: 'Coat',\n",
    " 5: 'Sandal',\n",
    " 6: 'Shirt',\n",
    " 7: 'Sneaker',\n",
    " 8: 'Bag',\n",
    " 9: 'Ankle boot'\n",
    "}\n",
    "\n",
    "class FashionMnist(Dataset):\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __init__(self, metadata_df,\n",
    "                 transform=None):\n",
    "        \n",
    "        self.metadata_df = metadata_df.copy()\n",
    "        self.transform = transform\n",
    "    \n",
    "    def load_image_and_target(self,index):\n",
    "        # .iloc is short for integer loc it returns a row of data based on its ored not index-value(if not the same)\n",
    "        oneimage = self.metadata_df.iloc[index]\n",
    "        image, y = PIL.Image.fromarray(\n",
    "            np.array(oneimage[1:]).reshape(28, 28).astype('uint8'), 'L'), oneimage[0]\n",
    "        return image, y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.load_image_and_target(index)\n",
    "        # We can transform the output images here, cast to torch data-format and/or do augmentations\n",
    "        X = self.transform(X)\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "    def collate_func(self, batch):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Understanding AdaptiveAveragePooling\n",
    "\n",
    "In a moment you will see that we change one of the pretrained models by swaping  the AveragePool operation for AdaptiveAveragePooling.\n",
    "\n",
    "The ideas is that adaptive pooling always returns our desired shape. This is useful if you want your NN to support different images shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 1, 1])\n",
      "torch.Size([1, 128, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn((1, 128, 8, 8))\n",
    "\n",
    "print(nn.AdaptiveAvgPool2d((1, 1))(tensor).shape)\n",
    "print(nn.AvgPool2d((3,3))(tensor).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting a pretrained model to be used for different number of classes\n",
    "By default many of the pretrained models require a input image of a given shape (usually 224x224x3) This is not our case so we need to chaged that, one way to do it is to change the network architecture of the model as described in Adaptive Pooling Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "\n",
    "def freeze_model(model):\n",
    "    model.eval()\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = False\n",
    "        \n",
    "cnn = resnet50(pretrained='imagenet')\n",
    "freeze_model(cnn)\n",
    "cnn.avgpool = nn.AdaptiveAvgPool2d(1) # This will allow to use different input sizes\n",
    "cnn.last_linear = nn.Sequential(nn.Linear(cnn.last_linear.in_features, 1024), \n",
    "                                nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your loss function / crieterion, optimizers and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0093)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR= 0.001\n",
    "BATCH_SIZE = 32\n",
    "DATASET_USAGE_SIZE = 0.15\n",
    "N_CLASSES = 10\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "OPTIMIZER = 'Adam' # one of ['ASGD','Adadelta', 'Adagrad','Adam', 'Adamax','LBFGS', 'RMSprop','Rprop','SGD',SparseAdam']\n",
    "optimizer = getattr(torch.optim, OPTIMIZER)(cnn.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn.to(DEVICE)\n",
    "\n",
    "# Create dataset loaders\n",
    "\n",
    "dftrain = pd.read_csv('fashion-mnist_train.csv').sample(frac=DATASET_USAGE_SIZE)\n",
    "dftest = pd.read_csv('fashion-mnist_test.csv').sample(frac=0.1)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.Resize(34), \n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize(mean=MEAN, std=STD)])\n",
    "fmnist_train = FashionMnist(dftrain, transform=transform_train)\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(34), \n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=MEAN, std=STD)\n",
    "                                    ])\n",
    "fmnist_test = FashionMnist(dftest, transform=transform_test)\n",
    "\n",
    "fmnist_train_dl = DataLoader(fmnist_train, batch_size=BATCH_SIZE)\n",
    "fmnist_test_dl = DataLoader(fmnist_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Lets try to use the criterion with dummy data\n",
    "yp = torch.randn(BATCH_SIZE, 10)\n",
    "yt = torch.randint(10, (BATCH_SIZE,))\n",
    "criterion(yp, yt.long())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: loss: 1.7752726389765738  acc: 0.333\n",
      "train: loss: 1.6652631864911982  acc: 0.36144444444444446\n",
      "test: loss: 1.880145104289055  acc: 0.396\n",
      "train: loss: 1.7705317957197484  acc: 0.41455555555555557\n",
      "test: loss: 1.832125482276082  acc: 0.405\n",
      "train: loss: 1.6364874144456634  acc: 0.42055555555555557\n",
      "test: loss: 2.1411976160407065  acc: 0.262\n",
      "train: loss: 1.9333824621737004  acc: 0.262\n",
      "test: loss: 2.020625954420306  acc: 0.331\n",
      "train: loss: 1.773457801586374  acc: 0.34455555555555556\n",
      "test: loss: 2.1138203261196615  acc: 0.36\n",
      "train: loss: 1.7836581937166578  acc: 0.3688888888888889\n",
      "test: loss: 2.5213026722073555  acc: 0.247\n",
      "train: loss: 2.1244265468120576  acc: 0.2712222222222222\n",
      "test: loss: 2.309702076256275  acc: 0.228\n",
      "train: loss: 1.906275503514619  acc: 0.25166666666666665\n",
      "test: loss: 2.2409773420989514  acc: 0.266\n",
      "train: loss: 1.7990968634025852  acc: 0.28455555555555556\n",
      "test: loss: 2.3875743684740884  acc: 0.249\n",
      "train: loss: 1.910337605150003  acc: 0.2698888888888889\n",
      "test: loss: 2.679888268508017  acc: 0.313\n",
      "train: loss: 2.0265287179013423  acc: 0.3268888888888889\n",
      "test: loss: 2.3050015975981952  acc: 0.189\n",
      "train: loss: 1.9059206449549206  acc: 0.19822222222222222\n",
      "test: loss: 2.5977604689598084  acc: 0.208\n",
      "train: loss: 2.1567505645555727  acc: 0.21666666666666667\n",
      "test: loss: 2.6328838422512635  acc: 0.244\n",
      "train: loss: 2.0864856200416932  acc: 0.254\n",
      "test: loss: 2.512178008913994  acc: 0.245\n",
      "train: loss: 2.100718595047338  acc: 0.26455555555555554\n",
      "test: loss: 3.069464923381805  acc: 0.286\n",
      "train: loss: 2.309353317198248  acc: 0.31622222222222224\n",
      "test: loss: 2.5436157293915747  acc: 0.236\n",
      "train: loss: 2.0293746068875  acc: 0.24466666666666667\n",
      "test: loss: 2.5484168331176043  acc: 0.289\n",
      "train: loss: 1.9043808963431252  acc: 0.30266666666666664\n",
      "test: loss: 2.5396175245046617  acc: 0.282\n",
      "train: loss: 1.8886213869030277  acc: 0.302\n",
      "test: loss: 2.2233129685521127  acc: 0.241\n",
      "train: loss: 1.7230480055874213  acc: 0.2541111111111111\n",
      "test: loss: 2.5019480050206186  acc: 0.218\n",
      "train: loss: 1.9688435797691346  acc: 0.23022222222222222\n",
      "test: loss: 2.1392591350078582  acc: 0.241\n",
      "train: loss: 1.8241355374968717  acc: 0.25266666666666665\n",
      "test: loss: 2.477624189019203  acc: 0.237\n",
      "train: loss: 1.968272759391202  acc: 0.23066666666666666\n",
      "test: loss: 3.032822879433632  acc: 0.22\n",
      "train: loss: 2.1534244687574966  acc: 0.21744444444444444\n",
      "test: loss: 2.377088082253934  acc: 0.233\n",
      "train: loss: 1.867282917295479  acc: 0.245\n",
      "test: loss: 3.016574444055557  acc: 0.202\n",
      "train: loss: 2.321489469521553  acc: 0.21522222222222223\n",
      "test: loss: 2.586260087668896  acc: 0.257\n",
      "train: loss: 1.8119351644936783  acc: 0.2637777777777778\n",
      "test: loss: 2.6909378948807716  acc: 0.249\n",
      "train: loss: 1.9067345981573065  acc: 0.249\n",
      "test: loss: 2.844646688044071  acc: 0.294\n",
      "train: loss: 2.0921277746895406  acc: 0.31577777777777777\n",
      "test: loss: 2.9348613707692217  acc: 0.289\n",
      "train: loss: 2.192529197044276  acc: 0.30533333333333335\n",
      "test: loss: 2.448958863377571  acc: 0.276\n",
      "train: loss: 1.7927025252964943  acc: 0.2906666666666667\n",
      "test: loss: 2.9178266189098356  acc: 0.227\n",
      "train: loss: 2.153287187225289  acc: 0.24944444444444444\n",
      "test: loss: 3.0450505121946336  acc: 0.236\n",
      "train: loss: 2.0917960763909034  acc: 0.25322222222222224\n",
      "test: loss: 2.744468251645565  acc: 0.235\n",
      "train: loss: 2.0534028814700864  acc: 0.2561111111111111\n",
      "test: loss: 2.9343664650917054  acc: 0.233\n",
      "train: loss: 2.254159589663148  acc: 0.24855555555555556\n",
      "test: loss: 2.622349165342777  acc: 0.217\n",
      "train: loss: 1.94166973825307  acc: 0.22155555555555556\n",
      "test: loss: 2.7775908927321433  acc: 0.204\n",
      "train: loss: 2.0030174574289057  acc: 0.21955555555555556\n",
      "test: loss: 2.963726717006415  acc: 0.251\n",
      "train: loss: 2.2044042938169506  acc: 0.2722222222222222\n",
      "test: loss: 2.476003143250942  acc: 0.201\n",
      "train: loss: 1.753080275347336  acc: 0.21888888888888888\n",
      "test: loss: 2.3804070542571134  acc: 0.274\n",
      "train: loss: 1.76669176467922  acc: 0.2892222222222222\n",
      "test: loss: 2.311622045516968  acc: 0.256\n",
      "train: loss: 1.8361655814157312  acc: 0.27211111111111114\n",
      "test: loss: 2.567470053136349  acc: 0.264\n",
      "train: loss: 1.9764081221724756  acc: 0.2861111111111111\n",
      "test: loss: 3.2877224138379098  acc: 0.213\n",
      "train: loss: 2.4519433639474832  acc: 0.23833333333333334\n",
      "test: loss: 2.7863237474025517  acc: 0.184\n",
      "train: loss: 2.1200536489725583  acc: 0.18922222222222224\n",
      "test: loss: 2.556333877682686  acc: 0.187\n",
      "train: loss: 1.9304049660162579  acc: 0.20622222222222222\n",
      "test: loss: 2.5009940692782404  acc: 0.196\n",
      "train: loss: 1.9521876697275373  acc: 0.20544444444444446\n",
      "test: loss: 2.5964162162812428  acc: 0.225\n",
      "train: loss: 1.881207051365447  acc: 0.234\n",
      "test: loss: 2.5046554867032973  acc: 0.216\n",
      "train: loss: 1.7634840643487866  acc: 0.23655555555555555\n",
      "test: loss: 2.826989906132221  acc: 0.193\n",
      "train: loss: 2.009405602204303  acc: 0.21155555555555555\n",
      "test: loss: 2.8778481652736665  acc: 0.182\n",
      "train: loss: 2.038207167673649  acc: 0.18422222222222223\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, print_info=False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        collect_results = []\n",
    "        collect_target = []\n",
    "        for batch in test_loader:\n",
    "            X, y = batch\n",
    "            X = X.repeat(1, 3, 1, 1)\n",
    "            X = X.to(DEVICE)\n",
    "            y = y.to(DEVICE).detach().cpu().numpy()\n",
    "            pred = cnn(X)\n",
    "            collect_results.append(pred.sigmoid().detach().cpu().numpy())\n",
    "            collect_target.append(y) \n",
    "        \n",
    "    preds_proba = np.concatenate(collect_results)\n",
    "    preds = preds_proba.argmax(axis=1)\n",
    "    targets = np.concatenate(collect_target)\n",
    "\n",
    "    ll = log_loss(targets, preds_proba)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    if print_info:\n",
    "        print(\"test log-loss: {}\".format(ll))\n",
    "        print(\"overall accuracy:  {}\".format(ac))\n",
    "#         print(classification_report(targets, preds))\n",
    "    model.train()\n",
    "#         freeze_model(model)\n",
    "        \n",
    "    return ll, acc\n",
    "            \n",
    "collect = []\n",
    "for epoch in range(50):\n",
    "    lossacc = 0\n",
    "    for i, batch in enumerate(fmnist_train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        X, y = batch\n",
    "        X = X.repeat(1, 3, 1, 1)\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        y_pred = cnn(X)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        collect.append(float(loss.detach().cpu().numpy()))  \n",
    "        \n",
    "    lltest, acctest = evaluate_model(cnn, fmnist_test_dl)\n",
    "    lltrain, acctrain = evaluate_model(cnn, fmnist_train_dl)\n",
    "    print(\"test: loss: {}  acc: {}\".format(lltest, acctest))\n",
    "    print(\"train: loss: {}  acc: {}\".format(lltrain, acctrain))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
