{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWithRestartsLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    r\"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
    "    .. math::\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
    "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
    "    When last_epoch=-1, sets initial lr as lr.\n",
    "    It has been proposed in\n",
    "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. This implements\n",
    "    the cosine annealing part of SGDR, the restarts and number of iterations multiplier.\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_max (int): Maximum number of iterations.\n",
    "        T_mult (float): Multiply T_max by this number after each restart. Default: 1.\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, T_mult=1):\n",
    "        self.T_max = T_max\n",
    "        self.T_mult = T_mult\n",
    "        self.restart_every = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.restarts = 0\n",
    "        self.restarted_at = 0\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def restart(self):\n",
    "        self.restart_every *= self.T_mult\n",
    "        self.restarted_at = self.last_epoch\n",
    "    \n",
    "    def cosine(self, base_lr):\n",
    "        return self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.step_n / self.restart_every)) / 2\n",
    "    \n",
    "    @property\n",
    "    def step_n(self):\n",
    "        return self.last_epoch - self.restarted_at\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.step_n >= self.restart_every:\n",
    "            self.restart()\n",
    "        return [self.cosine(base_lr) for base_lr in self.base_lrs]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(10, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "        \n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.01)\n",
    "\n",
    "import math\n",
    "sgdwr = CosineAnnealingWithRestartsLR(optimizer, 100, T_mult=1.5)\n",
    "\n",
    "lrs = []\n",
    "for i in range(1000):\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    sgdwr.step()\n",
    "    lrs.append(lr)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
